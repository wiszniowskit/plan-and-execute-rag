question,contexts,ground_truth,evolution_type,metadata,episode_done
What is the method for passing parameters between notebooks in Databricks?,"['.wheelhouse.zip"" }, { ""maven"": { ""coordinates"": ""org.jsoup:jsoup:1.7.2"", ""exclusions"": [ ""slf4j:slf4j"" ] } }, { ""pypi"": { ""package"": ""simplejson"", ""repo"": ""http://my-pypi-mirror.com"" } }, { ""cran"": { ""package"": ""ada"", ""repo"": ""https://cran.us.r-project.org"" } } ] }\n\n```\n\nFor more information, see the Databricks documentation for library types.\n\nPassing parameters between notebooks and pipelines\n\nYou can pass parameters to notebooks using baseParameters property in databricks activity.\n\nIn certain cases, you might require to pass back certain values from notebook back to the service, which can be used for control flow (conditional checks) in the service or be consumed by downstream activities (size limit is 2 MB).\n\nIn your notebook, you may call dbutils.notebook.exit(""returnValue"") and corresponding ""returnValue"" will be returned to the service.\n\nYou can consume the output in the service by using expression such as @{activity(\'databricks notebook activity name\').output.runOutput}.\n\n[!IMPORTANT] If you are passing JSON object you can retrieve values by appending property names. Example: @{activity(\'databricks notebook activity name\').output.runOutput.PropertyName}\n\nHow to upload a library in Databricks\n\nYou can use the Workspace UI:\n\nUse the Databricks workspace UI\n\nTo obtain the dbfs path of the library added using UI, you can use Databricks CLI.\n\nTypically the Jar libraries are stored under dbfs:/FileStore/jars while using the UI. You can list all through the CLI: databricks fs ls dbfs:/FileStore/job-jars\n\nOr you can use the Databricks CLI:\n\nFollow Copy the library using Databricks CLI\n\nUse Databricks CLI (installation steps)\n\nAs an example, to copy a JAR to dbfs: dbfs cp SparkPi-assembly-0.1.jar dbfs:/docs/sparkpi.jar']","You can pass parameters to notebooks in Databricks using the baseParameters property in the databricks activity. Additionally, you can call dbutils.notebook.exit(""returnValue"") in your notebook to return values back to the service, which can be used for control flow or consumed by downstream activities.",simple,[{'source': '/Users/tomasz/plan-and-execute-rag/docs/transform-data-databricks-notebook.md'}],True
What is the role of Apache Spark in the Azure Databricks platform?,"['title: Transform data with Databricks Notebook titleSuffix: Azure Data Factory & Azure Synapse description: Learn how to process or transform data by running a Databricks notebook in Azure Data Factory and Synapse Analytics pipelines. ms.custom: synapse author: nabhishek ms.author: abnarain ms.topic: conceptual ms.date: 05/15/2024\n\nTransform data by running a Databricks notebook\n\n[!INCLUDEappliesto-adf-asa-md]\n\nThe Azure Databricks Notebook Activity in a pipeline runs a Databricks notebook in your Azure Databricks workspace. This article builds on the data transformation activities article, which presents a general overview of data transformation and the supported transformation activities. Azure Databricks is a managed platform for running Apache Spark.\n\nYou can create a Databricks notebook with an ARM template using JSON, or directly through the Azure Data Factory Studio user interface. For a step-by-step walkthrough of how to create a Databricks notebook activity using the user interface, reference the tutorial Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory.\n\nAdd a Notebook activity for Azure Databricks to a pipeline with UI\n\nTo use a Notebook activity for Azure Databricks in a pipeline, complete the following steps:\n\nSearch for Notebook in the pipeline Activities pane, and drag a Notebook activity to the pipeline canvas.\n\nSelect the new Notebook activity on the canvas if it is not already selected.\n\nSelect the Azure Databricks tab to select or create a new Azure Databricks linked service that will execute the Notebook activity.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-activity.png"" alt-text=""Shows the UI for a Notebook activity."":::\n\nSelect the Settings tab and specify the notebook path to be executed on Azure Databricks, optional base parameters to be passed to the notebook, and any additional libraries to be installed on the cluster to execute the job.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-settings.png"" alt-text=""Shows the UI for the Settings tab for a Notebook activity."":::\n\nDatabricks Notebook activity definition\n\nHere is the sample JSON definition of a Databricks Notebook Activity:\n\njson { ""activity"": { ""name"": ""MyActivity"", ""description"": ""MyActivity description"", ""type"": ""DatabricksNotebook"", ""linkedServiceName"": { ""referenceName"": ""MyDatabricksLinkedservice"", ""type"": ""LinkedServiceReference"" }, ""typeProperties"": { ""notebookPath"": ""/Users/user@example.com/ScalaExampleNotebook"", ""baseParameters"": { ""inputpath"": ""input/folder1/"", ""outputpath"": ""output/"" }, ""libraries"": [ { ""jar"": ""dbfs:/docs/library.jar"" } ] } } }\n\nDatabricks Notebook activity properties\n\nThe following table describes the JSON properties used in the JSON definition:\n\nProperty Description Required name Name of the activity in the pipeline. Yes description Text describing what the activity does. No type For Databricks Notebook Activity, the activity type is DatabricksNotebook. Yes linkedServiceName Name of the Databricks Linked Service on which the Databricks notebook runs. To learn about this linked service, see Compute linked services article. Yes notebookPath The absolute path of the notebook to be run in the Databricks Workspace. This path must begin with a slash. Yes baseParameters An array of Key-Value pairs. Base parameters can be used for each activity run. If the notebook takes a parameter that is not specified, the default value from the notebook will be used. Find more on parameters in Databricks Notebooks . No libraries A list of libraries to be installed on the cluster that will execute the job. It can be an array of \\ . No\n\nSupported libraries for Databricks activities\n\nIn the above Databricks activity definition, you specify these library types: jar, egg, whl, maven, pypi, cran.\n\n```json { ""libraries"": [ { ""jar"": ""dbfs:/mnt/libraries/library.jar"" }, { ""egg"": ""dbfs:/mnt/libraries/library.egg"" }, { ""whl"": ""dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl"" }, { ""whl"": ""dbfs:/mnt/libraries/wheel-libraries']",The answer to given question is not present in context,simple,[{'source': '/Users/tomasz/plan-and-execute-rag/docs/transform-data-databricks-notebook.md'}],True
What is the process of data transformation in Azure Data Factory and how is it implemented using Databricks notebooks?,"['title: Transform data with Databricks Notebook titleSuffix: Azure Data Factory & Azure Synapse description: Learn how to process or transform data by running a Databricks notebook in Azure Data Factory and Synapse Analytics pipelines. ms.custom: synapse author: nabhishek ms.author: abnarain ms.topic: conceptual ms.date: 05/15/2024\n\nTransform data by running a Databricks notebook\n\n[!INCLUDEappliesto-adf-asa-md]\n\nThe Azure Databricks Notebook Activity in a pipeline runs a Databricks notebook in your Azure Databricks workspace. This article builds on the data transformation activities article, which presents a general overview of data transformation and the supported transformation activities. Azure Databricks is a managed platform for running Apache Spark.\n\nYou can create a Databricks notebook with an ARM template using JSON, or directly through the Azure Data Factory Studio user interface. For a step-by-step walkthrough of how to create a Databricks notebook activity using the user interface, reference the tutorial Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory.\n\nAdd a Notebook activity for Azure Databricks to a pipeline with UI\n\nTo use a Notebook activity for Azure Databricks in a pipeline, complete the following steps:\n\nSearch for Notebook in the pipeline Activities pane, and drag a Notebook activity to the pipeline canvas.\n\nSelect the new Notebook activity on the canvas if it is not already selected.\n\nSelect the Azure Databricks tab to select or create a new Azure Databricks linked service that will execute the Notebook activity.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-activity.png"" alt-text=""Shows the UI for a Notebook activity."":::\n\nSelect the Settings tab and specify the notebook path to be executed on Azure Databricks, optional base parameters to be passed to the notebook, and any additional libraries to be installed on the cluster to execute the job.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-settings.png"" alt-text=""Shows the UI for the Settings tab for a Notebook activity."":::\n\nDatabricks Notebook activity definition\n\nHere is the sample JSON definition of a Databricks Notebook Activity:\n\njson { ""activity"": { ""name"": ""MyActivity"", ""description"": ""MyActivity description"", ""type"": ""DatabricksNotebook"", ""linkedServiceName"": { ""referenceName"": ""MyDatabricksLinkedservice"", ""type"": ""LinkedServiceReference"" }, ""typeProperties"": { ""notebookPath"": ""/Users/user@example.com/ScalaExampleNotebook"", ""baseParameters"": { ""inputpath"": ""input/folder1/"", ""outputpath"": ""output/"" }, ""libraries"": [ { ""jar"": ""dbfs:/docs/library.jar"" } ] } } }\n\nDatabricks Notebook activity properties\n\nThe following table describes the JSON properties used in the JSON definition:\n\nProperty Description Required name Name of the activity in the pipeline. Yes description Text describing what the activity does. No type For Databricks Notebook Activity, the activity type is DatabricksNotebook. Yes linkedServiceName Name of the Databricks Linked Service on which the Databricks notebook runs. To learn about this linked service, see Compute linked services article. Yes notebookPath The absolute path of the notebook to be run in the Databricks Workspace. This path must begin with a slash. Yes baseParameters An array of Key-Value pairs. Base parameters can be used for each activity run. If the notebook takes a parameter that is not specified, the default value from the notebook will be used. Find more on parameters in Databricks Notebooks . No libraries A list of libraries to be installed on the cluster that will execute the job. It can be an array of \\ . No\n\nSupported libraries for Databricks activities\n\nIn the above Databricks activity definition, you specify these library types: jar, egg, whl, maven, pypi, cran.\n\n```json { ""libraries"": [ { ""jar"": ""dbfs:/mnt/libraries/library.jar"" }, { ""egg"": ""dbfs:/mnt/libraries/library.egg"" }, { ""whl"": ""dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl"" }, { ""whl"": ""dbfs:/mnt/libraries/wheel-libraries']","The process of data transformation in Azure Data Factory involves running a Databricks notebook within a pipeline. This is implemented by adding a Notebook activity for Azure Databricks to the pipeline, where you can specify the notebook path, base parameters, and any additional libraries needed for execution. The Azure Databricks Notebook Activity allows for the execution of a Databricks notebook in your Azure Databricks workspace, facilitating data processing and transformation.",simple,[{'source': '/Users/tomasz/plan-and-execute-rag/docs/transform-data-databricks-notebook.md'}],True
What role do pipeline activities play in the process of transforming data using Databricks notebooks in Azure Data Factory?,"['title: Transform data with Databricks Notebook titleSuffix: Azure Data Factory & Azure Synapse description: Learn how to process or transform data by running a Databricks notebook in Azure Data Factory and Synapse Analytics pipelines. ms.custom: synapse author: nabhishek ms.author: abnarain ms.topic: conceptual ms.date: 05/15/2024\n\nTransform data by running a Databricks notebook\n\n[!INCLUDEappliesto-adf-asa-md]\n\nThe Azure Databricks Notebook Activity in a pipeline runs a Databricks notebook in your Azure Databricks workspace. This article builds on the data transformation activities article, which presents a general overview of data transformation and the supported transformation activities. Azure Databricks is a managed platform for running Apache Spark.\n\nYou can create a Databricks notebook with an ARM template using JSON, or directly through the Azure Data Factory Studio user interface. For a step-by-step walkthrough of how to create a Databricks notebook activity using the user interface, reference the tutorial Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory.\n\nAdd a Notebook activity for Azure Databricks to a pipeline with UI\n\nTo use a Notebook activity for Azure Databricks in a pipeline, complete the following steps:\n\nSearch for Notebook in the pipeline Activities pane, and drag a Notebook activity to the pipeline canvas.\n\nSelect the new Notebook activity on the canvas if it is not already selected.\n\nSelect the Azure Databricks tab to select or create a new Azure Databricks linked service that will execute the Notebook activity.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-activity.png"" alt-text=""Shows the UI for a Notebook activity."":::\n\nSelect the Settings tab and specify the notebook path to be executed on Azure Databricks, optional base parameters to be passed to the notebook, and any additional libraries to be installed on the cluster to execute the job.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-settings.png"" alt-text=""Shows the UI for the Settings tab for a Notebook activity."":::\n\nDatabricks Notebook activity definition\n\nHere is the sample JSON definition of a Databricks Notebook Activity:\n\njson { ""activity"": { ""name"": ""MyActivity"", ""description"": ""MyActivity description"", ""type"": ""DatabricksNotebook"", ""linkedServiceName"": { ""referenceName"": ""MyDatabricksLinkedservice"", ""type"": ""LinkedServiceReference"" }, ""typeProperties"": { ""notebookPath"": ""/Users/user@example.com/ScalaExampleNotebook"", ""baseParameters"": { ""inputpath"": ""input/folder1/"", ""outputpath"": ""output/"" }, ""libraries"": [ { ""jar"": ""dbfs:/docs/library.jar"" } ] } } }\n\nDatabricks Notebook activity properties\n\nThe following table describes the JSON properties used in the JSON definition:\n\nProperty Description Required name Name of the activity in the pipeline. Yes description Text describing what the activity does. No type For Databricks Notebook Activity, the activity type is DatabricksNotebook. Yes linkedServiceName Name of the Databricks Linked Service on which the Databricks notebook runs. To learn about this linked service, see Compute linked services article. Yes notebookPath The absolute path of the notebook to be run in the Databricks Workspace. This path must begin with a slash. Yes baseParameters An array of Key-Value pairs. Base parameters can be used for each activity run. If the notebook takes a parameter that is not specified, the default value from the notebook will be used. Find more on parameters in Databricks Notebooks . No libraries A list of libraries to be installed on the cluster that will execute the job. It can be an array of \\ . No\n\nSupported libraries for Databricks activities\n\nIn the above Databricks activity definition, you specify these library types: jar, egg, whl, maven, pypi, cran.\n\n```json { ""libraries"": [ { ""jar"": ""dbfs:/mnt/libraries/library.jar"" }, { ""egg"": ""dbfs:/mnt/libraries/library.egg"" }, { ""whl"": ""dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl"" }, { ""whl"": ""dbfs:/mnt/libraries/wheel-libraries']","Pipeline activities, such as the Azure Databricks Notebook Activity, play a crucial role in transforming data by allowing users to run Databricks notebooks within Azure Data Factory and Synapse Analytics pipelines. This integration enables the execution of data transformation tasks using the capabilities of Databricks, which is a managed platform for running Apache Spark.",simple,[{'source': '/Users/tomasz/plan-and-execute-rag/docs/transform-data-databricks-notebook.md'}],True
What role do pipeline activities play in the process of transforming data using Databricks notebooks in Azure Data Factory?,"['title: Transform data with Databricks Notebook titleSuffix: Azure Data Factory & Azure Synapse description: Learn how to process or transform data by running a Databricks notebook in Azure Data Factory and Synapse Analytics pipelines. ms.custom: synapse author: nabhishek ms.author: abnarain ms.topic: conceptual ms.date: 05/15/2024\n\nTransform data by running a Databricks notebook\n\n[!INCLUDEappliesto-adf-asa-md]\n\nThe Azure Databricks Notebook Activity in a pipeline runs a Databricks notebook in your Azure Databricks workspace. This article builds on the data transformation activities article, which presents a general overview of data transformation and the supported transformation activities. Azure Databricks is a managed platform for running Apache Spark.\n\nYou can create a Databricks notebook with an ARM template using JSON, or directly through the Azure Data Factory Studio user interface. For a step-by-step walkthrough of how to create a Databricks notebook activity using the user interface, reference the tutorial Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory.\n\nAdd a Notebook activity for Azure Databricks to a pipeline with UI\n\nTo use a Notebook activity for Azure Databricks in a pipeline, complete the following steps:\n\nSearch for Notebook in the pipeline Activities pane, and drag a Notebook activity to the pipeline canvas.\n\nSelect the new Notebook activity on the canvas if it is not already selected.\n\nSelect the Azure Databricks tab to select or create a new Azure Databricks linked service that will execute the Notebook activity.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-activity.png"" alt-text=""Shows the UI for a Notebook activity."":::\n\nSelect the Settings tab and specify the notebook path to be executed on Azure Databricks, optional base parameters to be passed to the notebook, and any additional libraries to be installed on the cluster to execute the job.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-settings.png"" alt-text=""Shows the UI for the Settings tab for a Notebook activity."":::\n\nDatabricks Notebook activity definition\n\nHere is the sample JSON definition of a Databricks Notebook Activity:\n\njson { ""activity"": { ""name"": ""MyActivity"", ""description"": ""MyActivity description"", ""type"": ""DatabricksNotebook"", ""linkedServiceName"": { ""referenceName"": ""MyDatabricksLinkedservice"", ""type"": ""LinkedServiceReference"" }, ""typeProperties"": { ""notebookPath"": ""/Users/user@example.com/ScalaExampleNotebook"", ""baseParameters"": { ""inputpath"": ""input/folder1/"", ""outputpath"": ""output/"" }, ""libraries"": [ { ""jar"": ""dbfs:/docs/library.jar"" } ] } } }\n\nDatabricks Notebook activity properties\n\nThe following table describes the JSON properties used in the JSON definition:\n\nProperty Description Required name Name of the activity in the pipeline. Yes description Text describing what the activity does. No type For Databricks Notebook Activity, the activity type is DatabricksNotebook. Yes linkedServiceName Name of the Databricks Linked Service on which the Databricks notebook runs. To learn about this linked service, see Compute linked services article. Yes notebookPath The absolute path of the notebook to be run in the Databricks Workspace. This path must begin with a slash. Yes baseParameters An array of Key-Value pairs. Base parameters can be used for each activity run. If the notebook takes a parameter that is not specified, the default value from the notebook will be used. Find more on parameters in Databricks Notebooks . No libraries A list of libraries to be installed on the cluster that will execute the job. It can be an array of \\ . No\n\nSupported libraries for Databricks activities\n\nIn the above Databricks activity definition, you specify these library types: jar, egg, whl, maven, pypi, cran.\n\n```json { ""libraries"": [ { ""jar"": ""dbfs:/mnt/libraries/library.jar"" }, { ""egg"": ""dbfs:/mnt/libraries/library.egg"" }, { ""whl"": ""dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl"" }, { ""whl"": ""dbfs:/mnt/libraries/wheel-libraries']","Pipeline activities, specifically the Azure Databricks Notebook Activity, play a crucial role in transforming data by allowing users to run Databricks notebooks within Azure Data Factory and Synapse Analytics pipelines. This integration enables the execution of data transformation tasks using the capabilities of Databricks, which is a managed platform for running Apache Spark.",simple,[{'source': '/Users/tomasz/plan-and-execute-rag/docs/transform-data-databricks-notebook.md'}],True
Which Azure Data Factory activity runs a Databricks notebook for data transformation?,"['title: Transform data with Databricks Notebook titleSuffix: Azure Data Factory & Azure Synapse description: Learn how to process or transform data by running a Databricks notebook in Azure Data Factory and Synapse Analytics pipelines. ms.custom: synapse author: nabhishek ms.author: abnarain ms.topic: conceptual ms.date: 05/15/2024\n\nTransform data by running a Databricks notebook\n\n[!INCLUDEappliesto-adf-asa-md]\n\nThe Azure Databricks Notebook Activity in a pipeline runs a Databricks notebook in your Azure Databricks workspace. This article builds on the data transformation activities article, which presents a general overview of data transformation and the supported transformation activities. Azure Databricks is a managed platform for running Apache Spark.\n\nYou can create a Databricks notebook with an ARM template using JSON, or directly through the Azure Data Factory Studio user interface. For a step-by-step walkthrough of how to create a Databricks notebook activity using the user interface, reference the tutorial Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory.\n\nAdd a Notebook activity for Azure Databricks to a pipeline with UI\n\nTo use a Notebook activity for Azure Databricks in a pipeline, complete the following steps:\n\nSearch for Notebook in the pipeline Activities pane, and drag a Notebook activity to the pipeline canvas.\n\nSelect the new Notebook activity on the canvas if it is not already selected.\n\nSelect the Azure Databricks tab to select or create a new Azure Databricks linked service that will execute the Notebook activity.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-activity.png"" alt-text=""Shows the UI for a Notebook activity."":::\n\nSelect the Settings tab and specify the notebook path to be executed on Azure Databricks, optional base parameters to be passed to the notebook, and any additional libraries to be installed on the cluster to execute the job.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-settings.png"" alt-text=""Shows the UI for the Settings tab for a Notebook activity."":::\n\nDatabricks Notebook activity definition\n\nHere is the sample JSON definition of a Databricks Notebook Activity:\n\njson { ""activity"": { ""name"": ""MyActivity"", ""description"": ""MyActivity description"", ""type"": ""DatabricksNotebook"", ""linkedServiceName"": { ""referenceName"": ""MyDatabricksLinkedservice"", ""type"": ""LinkedServiceReference"" }, ""typeProperties"": { ""notebookPath"": ""/Users/user@example.com/ScalaExampleNotebook"", ""baseParameters"": { ""inputpath"": ""input/folder1/"", ""outputpath"": ""output/"" }, ""libraries"": [ { ""jar"": ""dbfs:/docs/library.jar"" } ] } } }\n\nDatabricks Notebook activity properties\n\nThe following table describes the JSON properties used in the JSON definition:\n\nProperty Description Required name Name of the activity in the pipeline. Yes description Text describing what the activity does. No type For Databricks Notebook Activity, the activity type is DatabricksNotebook. Yes linkedServiceName Name of the Databricks Linked Service on which the Databricks notebook runs. To learn about this linked service, see Compute linked services article. Yes notebookPath The absolute path of the notebook to be run in the Databricks Workspace. This path must begin with a slash. Yes baseParameters An array of Key-Value pairs. Base parameters can be used for each activity run. If the notebook takes a parameter that is not specified, the default value from the notebook will be used. Find more on parameters in Databricks Notebooks . No libraries A list of libraries to be installed on the cluster that will execute the job. It can be an array of \\ . No\n\nSupported libraries for Databricks activities\n\nIn the above Databricks activity definition, you specify these library types: jar, egg, whl, maven, pypi, cran.\n\n```json { ""libraries"": [ { ""jar"": ""dbfs:/mnt/libraries/library.jar"" }, { ""egg"": ""dbfs:/mnt/libraries/library.egg"" }, { ""whl"": ""dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl"" }, { ""whl"": ""dbfs:/mnt/libraries/wheel-libraries']",The Azure Databricks Notebook Activity in a pipeline runs a Databricks notebook for data transformation.,reasoning,[{'source': '/Users/tomasz/plan-and-execute-rag/docs/transform-data-databricks-notebook.md'}],True
How to connect a Databricks notebook to Azure Data Factory for data transformation?,"['title: Transform data with Databricks Notebook titleSuffix: Azure Data Factory & Azure Synapse description: Learn how to process or transform data by running a Databricks notebook in Azure Data Factory and Synapse Analytics pipelines. ms.custom: synapse author: nabhishek ms.author: abnarain ms.topic: conceptual ms.date: 05/15/2024\n\nTransform data by running a Databricks notebook\n\n[!INCLUDEappliesto-adf-asa-md]\n\nThe Azure Databricks Notebook Activity in a pipeline runs a Databricks notebook in your Azure Databricks workspace. This article builds on the data transformation activities article, which presents a general overview of data transformation and the supported transformation activities. Azure Databricks is a managed platform for running Apache Spark.\n\nYou can create a Databricks notebook with an ARM template using JSON, or directly through the Azure Data Factory Studio user interface. For a step-by-step walkthrough of how to create a Databricks notebook activity using the user interface, reference the tutorial Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory.\n\nAdd a Notebook activity for Azure Databricks to a pipeline with UI\n\nTo use a Notebook activity for Azure Databricks in a pipeline, complete the following steps:\n\nSearch for Notebook in the pipeline Activities pane, and drag a Notebook activity to the pipeline canvas.\n\nSelect the new Notebook activity on the canvas if it is not already selected.\n\nSelect the Azure Databricks tab to select or create a new Azure Databricks linked service that will execute the Notebook activity.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-activity.png"" alt-text=""Shows the UI for a Notebook activity."":::\n\nSelect the Settings tab and specify the notebook path to be executed on Azure Databricks, optional base parameters to be passed to the notebook, and any additional libraries to be installed on the cluster to execute the job.\n\n:::image type=""content"" source=""media/transform-data-databricks-notebook/notebook-settings.png"" alt-text=""Shows the UI for the Settings tab for a Notebook activity."":::\n\nDatabricks Notebook activity definition\n\nHere is the sample JSON definition of a Databricks Notebook Activity:\n\njson { ""activity"": { ""name"": ""MyActivity"", ""description"": ""MyActivity description"", ""type"": ""DatabricksNotebook"", ""linkedServiceName"": { ""referenceName"": ""MyDatabricksLinkedservice"", ""type"": ""LinkedServiceReference"" }, ""typeProperties"": { ""notebookPath"": ""/Users/user@example.com/ScalaExampleNotebook"", ""baseParameters"": { ""inputpath"": ""input/folder1/"", ""outputpath"": ""output/"" }, ""libraries"": [ { ""jar"": ""dbfs:/docs/library.jar"" } ] } } }\n\nDatabricks Notebook activity properties\n\nThe following table describes the JSON properties used in the JSON definition:\n\nProperty Description Required name Name of the activity in the pipeline. Yes description Text describing what the activity does. No type For Databricks Notebook Activity, the activity type is DatabricksNotebook. Yes linkedServiceName Name of the Databricks Linked Service on which the Databricks notebook runs. To learn about this linked service, see Compute linked services article. Yes notebookPath The absolute path of the notebook to be run in the Databricks Workspace. This path must begin with a slash. Yes baseParameters An array of Key-Value pairs. Base parameters can be used for each activity run. If the notebook takes a parameter that is not specified, the default value from the notebook will be used. Find more on parameters in Databricks Notebooks . No libraries A list of libraries to be installed on the cluster that will execute the job. It can be an array of \\ . No\n\nSupported libraries for Databricks activities\n\nIn the above Databricks activity definition, you specify these library types: jar, egg, whl, maven, pypi, cran.\n\n```json { ""libraries"": [ { ""jar"": ""dbfs:/mnt/libraries/library.jar"" }, { ""egg"": ""dbfs:/mnt/libraries/library.egg"" }, { ""whl"": ""dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl"" }, { ""whl"": ""dbfs:/mnt/libraries/wheel-libraries']","To connect a Databricks notebook to Azure Data Factory for data transformation, you need to add a Notebook activity for Azure Databricks to a pipeline. This involves searching for Notebook in the pipeline Activities pane, dragging a Notebook activity to the pipeline canvas, selecting the new Notebook activity, and then selecting the Azure Databricks tab to select or create a new Azure Databricks linked service that will execute the Notebook activity. You also need to specify the notebook path, optional base parameters, and any additional libraries to be installed on the cluster to execute the job.",reasoning,[{'source': '/Users/tomasz/plan-and-execute-rag/docs/transform-data-databricks-notebook.md'}],True
